# -*- coding: utf-8 -*-
"""[draft] Problem 3 submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1it_VHEMOJgJyRRVHIHkVaDVY7IcB_MdJ

# Problem 3

**Problem 3: Linearization of the feedforward layers in Neural Networks**

Consider a feedforward layer of the following form, where $\mathbf{W} \in \mathbb{R}^{d \times d}, \mathbf{x}, \mathbf{y} \in \mathbb{R}^d$ :
$$
\mathbf{y}=f(\mathbf{W} \mathbf{x})                        
$$

Assume that $f$ is a GELU function. Propose an algorithm to approximate it via the following "linearized variant", where $\Phi: \mathbb{R}^{d \times d} \rightarrow \mathbb{R}^{d \times m}, \Psi: \mathbb{R}^d \rightarrow \mathbb{R}^m$ are some functions (to be constructed by you):
$$
\mathbf{y}^{\prime}=\Phi(\mathbf{W}) \Psi(\mathbf{x}) .
$$

The approximation does not need to be unbiased. Can you propose the unbiased variant ?

# Creating a synthetic dataset
"""

import math
import matplotlib.pyplot as plt
import torch
from scipy.special import erf

import numpy as np

# Choosing d = 10 so we can see meaningful interactions between features
# via weights, but small enough to visualize and run efficient experiments.
d = 10

# Don't use np.random.rand(d, 1), which doesn't consider negatives since it
# takes a uniform distribution of [0,1]. Our dataset should consider negatives,
# since gelu accepts them.
#
# Mean = 0, std dev = 1 should be fine, since most inputs are normalized anyway.
x = np.random.normal(loc=0, scale=1, size=(d, 1))
print(x)

W = np.random.normal(loc=0, scale=1, size=(d, d))
print(W)

"""# GELU through tanh

TODO: Incorporate Jicheol's observations about tanh vs erf?
https://colab.research.google.com/drive/1YdChM136GCRsfg9z9N4YZzHasHshLQW5#scrollTo=zNvZw7xAWuy2

**Lisa: I've incorporated the graphical component here
"""

def gelu_tanh(x):
    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

# erf approximation
def gelu_erf(x):
    return 0.5 * x * (1 + erf(x/np.sqrt(2)))

"""# **Visualization of comparison between tanh and erf**"""

x = np.linspace(-5, 5, 1000)

plt.figure(figsize = (10,6))
plt.plot(x, gelu_tanh(x), label='GELU (tanh)')
plt.plot(x, gelu_erf(x), label='GELU (erf)', linestyle='--')
plt.xlabel('x')
plt.ylabel('GELU(x)')
plt.legend(['Tanh approximation', 'erf approximation'])
plt.show()

x_prime = W @ x
print(x_prime)

x_prime_tanh = gelu_tanh(x_prime)
print(x_prime_tanh)

"""# Biased estimation

A biased estimation doesn't get better with more attempts/trials, the estimation is deterministic (e.g. polynomial expansion, random features). In this section, we compare results with tanh/erf approximation.

Given $W$ ∈ $\mathbb{R}^{d\times d}$, $x$, $y$ ∈ $\mathbb{R}^{d}$, another way to approximate is using CDF:

$GELU(x)$ = $x$ $P(X <= x)$ = $x {\Phi}(x)$
"""

#This section is synonmous to tanh approximation, but using CDF instead. CDF is the cumulative distribution function for finding area under a standard normal function.
from scipy.stats import norm
phi_x = norm.cdf(x_prime)
print(phi_x)
GELU_x = x_prime * phi_x
print(GELU_x)

"""# **Approximating GELU using Monte Carlo simulation of sample size alpha = N and 1D variable x**"""

# Output is an unbiased estimator of the true value
def gelu_MC(x, N):
    success = 0
    for _ in range(N):
        z = np.random.normal(0, 1)
        if z <= x:
            success += 1
    return (success / N) * x

"""# **Below we show our biased approximation for a Taylor Series Expansion:**

Taylor polynomial Approximation (https://math.berkeley.edu/~scanlon/m16bs04/ln/16blec31ns/index.html)

let
$f(x)= \frac{1}{{\sqrt {2\pi } }}e^{ - \frac{{z^2 }}{2}} = .3989e^{ - 5z^2 }$ be the standard normal density function and

let
$F(x)={\frac{1}{\sqrt{2\pi}}}\int_{-\infty }^{x}e^{-t^{2}/2}\,dt $
be the standard normal cumulative distribution function

We compute a Taylor series expansion:

\begin{equation}
\begin{aligned}
G(x) & =\int \frac{1}{\sqrt{2 \pi}} e^{\frac{-1}{2} x^2} d x \\
& =\frac{1}{\sqrt{2 \pi}} \int \sum_{n=0}^{\infty} \frac{(-1)^n}{n!2^n} x^{2 n} d x \\
& =\frac{1}{\sqrt{2 \pi}} \sum_{n=0}^{\infty} \frac{(-1)^n}{n!2^n(2 n+1)} x^{2 n+1} \\
& =\frac{1}{\sqrt{2 \pi}}\left(x-\frac{1}{6} x^3+\frac{1}{40} x^5-\frac{1}{336} x^7+\cdots\right)
\end{aligned}
\end{equation}
"""

#for loop with large number of iterations
#https://pythonforundergradengineers.com/creating-taylor-series-functions-with-python.html
def gelu_taylor_basic(x):
  leading_coefficient = 1/math.sqrt(2*math.pi)
  term1 = leading_coefficient * (x)
  term2 = -leading_coefficient * (x**3)/6
  term3 = leading_coefficient * (x**5)/40
  term4 = -leading_coefficient * (x**7)/336
  term5 = leading_coefficient * (x**9)/2480
  term6 = -leading_coefficient * (x**11)/40320
  term7 = leading_coefficient * (x**13)/6227040
  return x*(0.5 + term1 + term2 + term3 + term4)

#Taylor approximation using n terms, recursion
def gelu_taylor_recursive(x, n):
  leading_coefficient = 1/math.sqrt(2*math.pi)
  if n == 0:
    #first term means that there is only x
    #this is the base case
    return 0.5+leading_coefficient *((-1)**n) * (x**(2*n+1))/(math.factorial(n)*(2**n)*(2*n+1))
  else:
    #Recursion step, adding terms iteratively
    return gelu_taylor_recursive(x, n-1) + leading_coefficient * (-1)**n / (math.factorial(n) * (2**n) * (2*n+1)) * x**(2*n+1)

# Taylor approximation using n terms, for-loop
def gelu_taylor(x,n):
  leading_coefficient = 1/np.sqrt(2*np.pi)
  sum = 0
  for i in range(n):
    sum += (-1)**i / (math.factorial(i)* (2**i) * (2*i+1)) * x**(2*i+1)
  return x * (0.5+leading_coefficient*sum)

#sample input

x = 0.5
n = 100
print("Monte Carlo approximation:", gelu_MC(x,n)) #this value changes every time due to random sampling
print("Tanh approximation: ", gelu_tanh(x))
print("Erf approximation: ", gelu_erf(x))
print("Taylor polynomial: ", gelu_taylor(x,n))
print("Base case Taylor polynomial: ", gelu_taylor_basic(x)) # Adding terms by brute force
print("Taylor polynomial, recursive case", x * gelu_taylor_recursive(x,n)) #need to multiply by x because of the function is taylor of standard normal cdf

"""# **Activation functions show similar trends across five approximations**"""

x = np.linspace(-5, 5, 1000)

plt.figure(figsize = (10,6))
plt.plot(x, gelu_tanh(x), label='GELU (tanh)')
plt.plot(x, gelu_erf(x), label='GELU (erf)', linestyle='--')
plt.plot(x, gelu_taylor(x,100), label='GELU (taylor)', linestyle='-')
plt.plot(x, x*gelu_taylor_recursive(x, 100), label='GELU (taylor_recursive)')
plt.plot(x, [gelu_MC(i,100) for i in x], label='GELU (MC)', linestyle='-')
plt.xlabel('x')
plt.ylabel('GELU(x)')
plt.legend(['Tanh approximation', 'erf approximation', 'Taylor approximation', 'Taylor recursive approximation', 'Monte Carlo approximation'])
plt.show()

"""# **Extremity Case**



"""

x = np.linspace(-5, 5, 1000)
plt.figure(figsize = (10,5))
plt.plot(x, x*gelu_taylor_recursive(x,n=5), label='GELU Taylor Polynomial Approximation Curve')
plt.plot(x, gelu_erf(x), label='GELU (erf)', linestyle='--')
plt.legend(['GELU Taylor Polynomial Approximation Curve', 'GELU (erf)'])
plt.show()

"""# GELU Interpretation
\begin{equation}
\begin{aligned}
&\begin{aligned}
\operatorname{GELU}(z) & =x \Phi(x), x \in \mathbb{R} \\
& =x \cdot P(z \leq x), Z \sim N(0,1) \\
& =x \cdot E[1\{z \leq x\}], \text { where } 1(x)= \begin{cases}1, & z \leq x \\
0, & z>x \\\end{cases}
& \approx x \cdot \frac{1}{N} \sum_{i=1}^N 1_{z_i \leq x} \end{aligned}\\
&\text { and as } N \rightarrow \infty \text { this converges to } \operatorname{GELU}(x)
\end{aligned}
\end{equation}

# Given W_x is interpreted as:


\begin{equation}
w_x=
\begin{bmatrix}
    \omega_{x_{1,1}} & \omega_{x_{1,2}} & \omega_{x_{1,3}} & \dots  & \omega_{x_{1,n}} \\
    \omega_{x_{2,1}} & \omega_{x_{2,2}} & \omega_{x_{2,3}} & \dots  & \omega_{x_{2,n}} \\
    \vdots & \vdots & \vdots & \ddots & \vdots \\
    \omega_{x_{d,1}} & \omega_{x_{d,2}} & \omega_{x_{d,3}} & \dots  & \omega_{x_{d,n}}
\end{bmatrix}
\end{equation}

**We want to find $\Phi(\omega)$ and $\psi(x)$ s.t.**
$$
\begin{array}{cl}
f\left(w_x\right)_{i j}=\left(w_x\right)_{i j} \cdot \Phi\left(\left(w_x\right)_{i j}\right) & i=1, \ldots R \\ & j =1, \ldots D \\
\text { or } f\left(w_x\right)=w_x \odot \Phi\left(w_x\right) &
\end{array}
$$

$ \odot $ here is an elementwise product. total of RD elementwise approximations.

\begin{aligned}
\therefore f\left(w_x\right)_{i j} & =\operatorname{GELU}\left(\left(w_x\right)_{i j}\right) \\
& \approx \frac{1}{N} \sum\left(w_x\right)_{i j} \cdot 1 z_k^{i j} \leq\left(w_x\right)_{i j}, z_k^{(i j)} \backsim N(0,1)
\end{aligned}

**So we would like to find a solution to accomodate W_x:**
$$
G\left(w_x{(i, j)}\right) \approx \frac{1}{\sqrt{2 \pi}} \sum_{n=0}^{\infty} \frac{(-1)^n}{2^n n!(2 n+1)}\left(w_x{(i,j)}\right)^{2 n+1}
$$

# Unbiased estimation

TODO: These get better with more attempts (e.g. Monte Carlo-based approximation).
"""